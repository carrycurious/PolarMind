# -*- coding: utf-8 -*-
"""Yet another copy of labse(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17XEawdLZcuYv4OYNyyt5dIrlBoVCaYEC
"""

from pathlib import Path
import zipfile
import glob
import os

import numpy as np
import torch
import torch.nn as nn
import pandas as pd
from torch.utils.data import DataLoader
from datasets import Dataset
from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding, get_linear_schedule_with_warmup
from sklearn.metrics import f1_score, accuracy_score
from tqdm.auto import tqdm

from utils.data_utils import ensure_extracted, load_multilingual_data, get_difficulty_scores

# --- 0. PATHS / DATASET SETUP ---
PROJECT_ROOT = Path(__file__).resolve().parent
SUBTASK1_DIR = PROJECT_ROOT / "dataset" / "subtask1"
RAW_ZIP_PATH = SUBTASK1_DIR / "rawdata" / "subtask1.zip"
EXTRACT_ROOT = SUBTASK1_DIR / "raw_extracted"
CLDATA_DIR = SUBTASK1_DIR / "CLdata"
CLDATA_DIR.mkdir(parents=True, exist_ok=True)

# Ensure the raw zip is extracted under EXTRACT_ROOT (idempotent)
ensure_extracted(RAW_ZIP_PATH, EXTRACT_ROOT)

# --- 1. CONFIGURATION ---
MODEL_NAME = "FacebookAI/xlm-roberta-base"
MAX_LENGTH = 256
BATCH_SIZE = 32
NUM_EPOCHS = 3
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- 2. MODEL DEFINITION ---
class xlmClassifier(nn.Module):
    def __init__(self, model_name, num_labels=2, num_layers_to_aggregate=4):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(model_name)
        self.hidden_size = self.encoder.config.hidden_size
        self.num_layers = num_layers_to_aggregate

        # Learnable weights for aggregating the selected layers
        self.layer_weights = nn.Parameter(torch.ones(self.num_layers))

        self.classifier = nn.Sequential(
            nn.Linear(self.hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, num_labels)
        )

    def clspooling(self, hidden_state, attention_mask=None):
        # Extract the first token ([CLS] or <s>)
        return hidden_state[:, 0, :]

    def forward(self, input_ids, attention_mask):
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )

        # Extract last N hidden states (tuple of layers)
        selected_layers = outputs.hidden_states[-self.num_layers:]

        # Pool each layer using CLS pooling
        pooled_layers = [self.clspooling(layer) for layer in selected_layers]
        stacked_pools = torch.stack(pooled_layers, dim=-1) # [batch, hidden, num_layers]

        # Normalized weighted sum
        layer_weights_normalized = torch.softmax(self.layer_weights, dim=0)
        weighted_output = torch.matmul(stacked_pools, layer_weights_normalized)

        logits = self.classifier(weighted_output)
        return logits

# --- 3. DATA PREPARATION ---

# Load multilingual train/dev data from the extracted zip under dataset/subtask1
train_pattern = str(EXTRACT_ROOT / "**" / "train" / "*.csv")
dev_pattern = str(EXTRACT_ROOT / "**" / "dev" / "*.csv")
train_dataset, test_dataset = load_multilingual_data(train_pattern, dev_pattern)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def tokenize_fn(batch):
    return tokenizer(batch["text"], truncation=True, max_length=MAX_LENGTH)

cols_to_remove = [c for c in train_dataset.column_names if c not in ["polarization"]]
train_dataset = train_dataset.map(tokenize_fn, batched=True, remove_columns=cols_to_remove)
test_dataset = test_dataset.map(tokenize_fn, batched=True, remove_columns=cols_to_remove)

train_dataset = train_dataset.rename_column("polarization", "labels")
test_dataset = test_dataset.rename_column("polarization", "labels")

train_dataset.set_format("torch")
test_dataset.set_format("torch")

data_collator = DataCollatorWithPadding(tokenizer)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=data_collator)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=data_collator)

# --- 4. OPTIMIZER & SCHEDULER ---
model = xlmClassifier(MODEL_NAME).to(DEVICE)

# Different learning rates for different parts of the model
optimizer_groups = [
    {"params": model.encoder.parameters(), "lr": 1e-5, "weight_decay": 0.01},
    {"params": model.classifier.parameters(), "lr": 1e-4, "weight_decay": 0.01},
    {"params": [model.layer_weights], "lr": 1e-3, "weight_decay": 0.0}
]

optimizer = torch.optim.AdamW(optimizer_groups)

# (multilingual data loading is handled via utils.load_multilingual_data above)

def tokenize_fn(batch):
      return tokenizer(batch["text"], truncation=True, max_length=256)
cols_to_remove = [c for c in train_dataset.column_names if c not in ["polarization"]]
train_dataset = train_dataset.map(tokenize_fn, batched=True, remove_columns=cols_to_remove)
test_dataset= test_dataset.map(tokenize_fn, batched=True, remove_columns=cols_to_remove)

train_dataset = train_dataset.rename_column("polarization", "labels")
test_dataset= test_dataset.rename_column("polarization", "labels")






train_dataset.set_format("torch")
test_dataset.set_format("torch")



tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

data_collator = DataCollatorWithPadding(tokenizer)


train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=data_collator)
test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=data_collator)

optimizer = torch.optim.AdamW(optimizer_groups,betas=(0.9,0.98),weight_decay=0.01)
from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding, get_cosine_schedule_with_warmup

num_epochs = 1
num_train_steps = len(train_loader) * num_epochs
num_warmup_steps = int(0.1 * num_train_steps)

scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_train_steps
)

print("Scheduler: warmup + cosine decay to 0 (annealing).")

len(train_loader)

import wandb
import torch # Import torch to use torch.device

# Define DEVICE here to ensure it's available
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

EPOCHS = 1
l1_lambda = 0
# Removed stray 'd'
MAX_LENGTH = 128
BATCH_SIZE = 16
LEARNING_RATE = 2e-5
wandb.init(project="polarization-classification", config={
    "learning_rate": LEARNING_RATE,
    "batch_size": BATCH_SIZE,
    "epochs": EPOCHS,
    "model_name": MODEL_NAME,
    "max_length": MAX_LENGTH
})

# Optional: Watch the model for gradients and parameters
criterion = nn.CrossEntropyLoss()

for epoch in range(EPOCHS):
    # Training
    model.train()
    total_loss = 0
    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1} [Train]")):
        batch = {k: v.to(DEVICE) for k, v in batch.items()}

        optimizer.zero_grad()
        logits = model(batch['input_ids'], batch['attention_mask'])
        # Note: EuroBERTClassifier does not have layer_weights, so this line will cause an error if l1_lambda > 0
        # If you intend to use L1 regularization, you will need to re-implement model.layer_weights or remove this part.
        # For now, keeping l1_lambda = 0 will prevent issues.
        # l1_penalty = model.layer_weights.abs().sum() # This line should be removed or commented out if EuroBERTClassifier is used and l1_lambda > 0



        baseloss = criterion(logits, batch['labels'])
        loss = baseloss # Assuming l1_lambda is 0, so no L1 penalty is added

        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

        total_loss += loss.item()

        # Log training loss every 100 batches to wandb
        if (batch_idx + 1) % 100 == 0:
            wandb.log({"train/batch_loss": loss.item(), "global_step": epoch * len(train_loader) + batch_idx})

    # Validation
    model.eval()
    all_preds, all_labels = [], []
    val_loss = 0
    with torch.no_grad():
        for batch in tqdm(test_loader, desc=f"Epoch {epoch+1} [Val]"):
            batch = {k: v.to(DEVICE) for k, v in batch.items()}
            logits = model(batch['input_ids'], batch['attention_mask'])

            val_loss += criterion(logits, batch['labels']).item()
            preds = torch.argmax(logits, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(batch['labels'].cpu().numpy())

    avg_train_loss = total_loss / len(train_loader)
    avg_val_loss = val_loss / len(test_loader)
    f1 = f1_score(all_labels, all_preds, average='macro')
    acc = accuracy_score(all_labels, all_preds)

    # Log epoch-level metrics to wandb
    wandb.log({
        "train/avg_loss": avg_train_loss,
        "val/avg_loss": avg_val_loss,
        "val/f1_score": f1,
        "val/accuracy": acc,
        "epoch": epoch
    })

    print(f"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | F1: {f1:.4f} | Acc: {acc:.4f}")


wandb.finish()

import numpy as np
import zipfile
import os
import pandas as pd
from datasets import Dataset, concatenate_datasets
import glob
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
import torch.nn as nn # Ensure nn is imported for CrossEntropyLoss

# --- 1. RECONSTRUCT THE FULL ORIGINAL DATAFRAME FOR ALL LANGUAGES ---
# Paths to your data (from the extracted subtask1 archive under dataset/subtask1)
data_path = str(EXTRACT_ROOT / "**" / "train" / "*.csv")

all_train_dfs = []
train_files = glob.glob(data_path, recursive=True)
for file in train_files:
    df = pd.read_csv(file)
    all_train_dfs.append(df)

# This DataFrame will contain all original data (id, text, polarization) for all languages
df_full_train_original = pd.concat(all_train_dfs, ignore_index=True)

# --- 2. PREPARE A DATALOADER FOR SCORING (CRITICALLY, NO SHUFFLE) ---
# Convert the full original DataFrame to a Hugging Face Dataset
full_train_hf_dataset = Dataset.from_pandas(df_full_train_original)

# Define the tokenization function (using the global tokenizer and MAX_LENGTH)
# Note: MAX_LENGTH was 256 in model definition (fgcrjFs4U0ly), 128 in wandb init (wL-Lk00uYiKN)
# Using 256 for consistency with initial model setup.
MAX_LENGTH = 256 # Ensure MAX_LENGTH is set for tokenization

def tokenize_fn_for_scoring(batch):
    return tokenizer(batch["text"], truncation=True, max_length=MAX_LENGTH)

# Tokenize the dataset and rename 'polarization' to 'labels'
# Keep 'id' and 'text' columns for later use, only remove others if present
full_train_hf_dataset_tokenized = full_train_hf_dataset.map(
    tokenize_fn_for_scoring,
    batched=True,
    remove_columns=[col for col in full_train_hf_dataset.column_names if col not in ["text", "polarization", "id"]]
)
full_train_hf_dataset_tokenized = full_train_hf_dataset_tokenized.rename_column("polarization", "labels")
full_train_hf_dataset_tokenized.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

# Create a DataLoader specifically for scoring (shuffle=False is crucial)
train_loader_for_scoring = DataLoader(
    full_train_hf_dataset_tokenized,
    batch_size=BATCH_SIZE, # Use the globally defined BATCH_SIZE
    shuffle=False, # IMPORTANT: Do NOT shuffle when calculating scores to align with original DataFrame
    collate_fn=data_collator # Use the globally defined data_collator
)

# --- 3. GET DIFFICULTY SCORES ---
scores = get_difficulty_scores(model, train_loader_for_scoring, DEVICE)

# --- 4. ADD SCORES TO THE ORIGINAL DATAFRAME ---
# df_full_train_original now has 73681 rows, matching scores length
df_full_train_original['difficulty_score'] = scores

# --- 5. SORT BY DIFFICULTY AND SPLIT INTO 3 EQUAL PARTS ---
df_sorted = df_full_train_original.sort_values(by='difficulty_score').reset_index(drop=True)
n = len(df_sorted)
split_idx = n // 3

df_easy = df_sorted.iloc[:split_idx]
df_medium = df_sorted.iloc[split_idx : 2*split_idx]
df_hard = df_sorted.iloc[2*split_idx:]

# --- 6. SAVE AND ZIP THE FILES ---
files = {'easy.csv': df_easy, 'medium.csv': df_medium, 'hard.csv': df_hard}
zip_path = CLDATA_DIR / "curriculum_datasets.zip"

with zipfile.ZipFile(zip_path, 'w') as zipf:
    for filename, df in files.items():
        # Save the relevant columns: id, text, polarization, and the new difficulty_score
        df[['id', 'text', 'polarization', 'difficulty_score']].to_csv(filename, index=False)
        zipf.write(filename)
        os.remove(filename) # Clean up local csv

print(f"Successfully created {zip_path} with equal-sized splits!")